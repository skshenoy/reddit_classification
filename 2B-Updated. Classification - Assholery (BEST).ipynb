{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assholery Highlights\n",
    "## Showcasing the best models for each section from the `2B. Classification - Assholery (FULL)` notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "### [Reading In Data](#r/AmITheAsshole)\n",
    "### [Part 1 Highlights: Using `op_text`](#Part-One) and [Results](#Part-One-(op_text)-Best-Results)\n",
    "### [Part 2 Highlights: Using `comments`](#Part-Two) and [Results](#Part-Two-(comments)-Best-Results)\n",
    "### [Part 3 Highlights: Using `op_text` and `comments`](#Part-Three) and [Results](#Part-Three-(all_text)-Best-Results)\n",
    "### [Overall Findings](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "import nltk\n",
    "import regex as re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verdict(all_comments):\n",
    "    \"\"\"\n",
    "    Counts up all of the judgements (using the subreddit's rules; must be all-caps for their bot to pick it up)\n",
    "    and gets the verdict for the given post.\n",
    "    Returns the verdict (the judgement with the most votes) and a dictionary of all the votes by option.\n",
    "    \"\"\"\n",
    "    opinions = {\n",
    "        'NTA':0,\n",
    "        'YTA':0,\n",
    "        'ESH':0,\n",
    "        'NAH':0,\n",
    "        'INFO':0,\n",
    "        'YWBTA':0\n",
    "    }\n",
    "    \n",
    "    for option in opinions.keys():\n",
    "        opinions[option] = all_comments.count(option)\n",
    "    \n",
    "    return list(sorted(opinions.items(), key=operator.itemgetter(1), reverse=True))[0][0], opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_automod(comms):\n",
    "    \"\"\"\n",
    "    Removes the automod comment(s) that are pinned to the top of the post\n",
    "    (meaning they're always at the beginning of the string of concatenated comments here).\n",
    "    \"\"\"\n",
    "    automod_mess = \"(/message/compose/?to=/r/AmItheAsshole) if you have any questions or concerns.\"\n",
    "    automod_end = comms.find(automod_mess)\n",
    "    while automod_end != -1:\n",
    "        comms = comms[automod_end+len(automod_mess):]\n",
    "        automod_end = comms.find(automod_mess)\n",
    "    return comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_judgements(text):\n",
    "    \"\"\"\n",
    "    Removes any variation of the common judgements in the text of the comments, because leaving that in would be\n",
    "    a dead giveaway to the classifier.\n",
    "    \"\"\"\n",
    "    to_remove = ['Wibta', 'Aita', 'Nta', 'Esh', 'Yta', 'Ywbta', 'Nah']\n",
    "    to_remove += [tr.lower() for tr in to_remove]\n",
    "    to_remove += [tr.upper() for tr in to_remove]\n",
    "    for tr in to_remove:\n",
    "        text = text.replace(tr, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_post(post):\n",
    "    \"\"\"\n",
    "    Does basic cleaning of the text. Removes non-alphabetical characters and transforms to lowercase.\n",
    "    \"\"\"\n",
    "    return \" \".join(re.sub(\"[^a-zA-Z]\", \" \", str(post)).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(y_test, preds):\n",
    "    \"\"\"\n",
    "    Takes in actual y-values and predicted y-values and prints out two Markdown tables:\n",
    "    a formatted confusion matrix; and a table of five computed classification metrics.\n",
    "    \"\"\"\n",
    "    display(Markdown('#### Confusion Matrix'))\n",
    "    display(pd.DataFrame(confusion_matrix(y_test, preds), columns=['predicted NTA', 'predicted TA'],\n",
    "            index=['actual NTA', 'actual TA']))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    \n",
    "    accuracy = round(((tn + tp) / (tn + tp + fn + fp)), 3)\n",
    "    \n",
    "    precision = round((tp / (tp + fp)), 3)\n",
    "    \n",
    "    sensitivity = round((tp / (tp + fn)), 3)\n",
    "    \n",
    "    specificity = round((tn / (tn + fp)), 3)\n",
    "    \n",
    "    misclassification_rate = round(((fp + fn) / (tn + tp + fn + fp)), 3)\n",
    "    \n",
    "    performance = f\"\"\"| Metric  | Score\n",
    "|--------|--------------------\n",
    "| Accuracy | {accuracy} |\n",
    "| Precision | {precision} |\n",
    "| Sensitivity | {sensitivity} |\n",
    "| Specificity | {specificity} |\n",
    "| Misclassification Rate | {misclassification_rate} |\"\"\"\n",
    "    \n",
    "    display(Markdown('#### Performance Metrics'))\n",
    "    display(Markdown(performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a scorer to pass in to GridSearchCV() when I want to compare based on this instead of on accuracy score.\n",
    "# FIXED: needs_proba=True, affects how the scorer is instantiated and what happens inside the GridSearch\n",
    "# (remember the stackoverflow question about this exact problem)\n",
    "roc_auc = make_scorer(roc_auc_score, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(fpr, tpr, thresholds):\n",
    "    \"\"\"\n",
    "    Plots out the ROC curve and AUC.\n",
    "    (adapted from sklearn documentation example)\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC-AUC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_pipe_performance(gspipe, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Prints out a number of informative messages about a GridSearchCV object after it has been fit.\n",
    "    Includes: best score, best params (for the best estimator), training and test scores, the scorer,\n",
    "    the baseline (i.e., normalized actual y-value counts), and also calls performance_metrics().\n",
    "    Also calculates and plots out the ROC curve and AUC (by calling plot_roc_auc()).\n",
    "    \"\"\"\n",
    "    print(\"Best score:\", gspipe.best_score_)\n",
    "    print()\n",
    "    print(\"Best params:\")\n",
    "    for k, v in gspipe.best_params_.items():\n",
    "        print(\"\\t\"+k, v)\n",
    "        print()\n",
    "    print()\n",
    "    print(\"Training score:\", gspipe.score(X_train, y_train))\n",
    "    print(\"Test score:\", gspipe.score(X_test, y_test))\n",
    "    print(\"Scorer:\", gspipe.scorer_)\n",
    "    print()\n",
    "    print(\"Baseline:\")\n",
    "    print(y_test.value_counts(normalize=True))\n",
    "    print()\n",
    "    preds = gspipe.predict(X_test)\n",
    "    print(preds[:20])\n",
    "    performance_metrics(y_test, preds)\n",
    "    pred_probas_pos = gspipe.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probas_pos)\n",
    "    plot_roc_auc(fpr, tpr, thresholds)\n",
    "    print(roc_auc_score(y_test, pred_probas_pos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r/AmITheAsshole\n",
    "[**return to top of notebook**](#Table-of-Contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2002, 4)\n",
      "id object\n",
      "title object\n",
      "selftext object\n",
      "comments object\n",
      "id          0\n",
      "title       0\n",
      "selftext    0\n",
      "comments    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>dgjarh</td>\n",
       "      <td>AITA for not wanting my dads new gf to sing at...</td>\n",
       "      <td>So I (28F) am getting married next year. \\n\\nI...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>dgjgap</td>\n",
       "      <td>AITA for “looking poor”?</td>\n",
       "      <td>Soo my family used to be really poor but my pa...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>dgjvfw</td>\n",
       "      <td>AITA for calling my family out on their eating...</td>\n",
       "      <td>My brother is 15, turning 16 soon.  He gets no...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>dgk74h</td>\n",
       "      <td>AITA for using the street parking in front of ...</td>\n",
       "      <td>I strongly dislike my neighbors and that is ma...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>dgkgu6</td>\n",
       "      <td>AITA for refusing to cook my boyfriends steak ...</td>\n",
       "      <td>I’ve been with my boyfriend for a couple years...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  dgjarh  AITA for not wanting my dads new gf to sing at...   \n",
       "1  dgjgap                           AITA for “looking poor”?   \n",
       "2  dgjvfw  AITA for calling my family out on their eating...   \n",
       "3  dgk74h  AITA for using the street parking in front of ...   \n",
       "4  dgkgu6  AITA for refusing to cook my boyfriends steak ...   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  So I (28F) am getting married next year. \\n\\nI...   \n",
       "1  Soo my family used to be really poor but my pa...   \n",
       "2  My brother is 15, turning 16 soon.  He gets no...   \n",
       "3  I strongly dislike my neighbors and that is ma...   \n",
       "4  I’ve been with my boyfriend for a couple years...   \n",
       "\n",
       "                                            comments  \n",
       "0  \\nIf you want your comment to count toward jud...  \n",
       "1  \\nIf you want your comment to count toward jud...  \n",
       "2  \\nIf you want your comment to count toward jud...  \n",
       "3  \\nIf you want your comment to count toward jud...  \n",
       "4  \\nIf you want your comment to count toward jud...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aita = pd.read_csv('./datasets/with_comments/r_AmITheAsshole_with_comments.csv')\n",
    "aita = pd.read_csv('/Users/shreya/DSI/projects/project_3fake/datasets/with_comments/r_AmITheAsshole_with_comments.csv')\n",
    "print(aita.shape)\n",
    "for c in aita.columns:\n",
    "    print(c, aita[c].dtype)\n",
    "print(aita.isnull().sum())\n",
    "aita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NTA      1547\n",
       "YTA       375\n",
       "NAH        54\n",
       "ESH        25\n",
       "YWBTA       1\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita['verdict'], aita['opinions'] = zip(*aita['comments'].apply(get_verdict))\n",
    "aita['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1547\n",
       " 1     375\n",
       "-1      80\n",
       "Name: is_TA, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita['is_TA'] = aita['verdict'].map({\n",
    "    'YTA':1,\n",
    "    'ESH':-1,\n",
    "    'NTA':0,\n",
    "    'NAH':-1,\n",
    "    'YWBTA':-1\n",
    "})\n",
    "aita['is_TA'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1547\n",
      "1     375\n",
      "Name: is_TA, dtype: int64\n",
      "\n",
      "0    0.804891\n",
      "1    0.195109\n",
      "Name: is_TA, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "aita = aita.loc[aita['is_TA'] != -1].copy()\n",
    "print(aita['is_TA'].value_counts())\n",
    "print()\n",
    "print(aita['is_TA'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aita['clean_comments'] = aita['comments'].map(remove_automod)\n",
    "aita['clean_comments'] = aita['clean_comments'].map(remove_judgements)\n",
    "aita['clean_op_text'] = aita['title'].map(remove_judgements) + \" \" + aita['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>comments</th>\n",
       "      <th>verdict</th>\n",
       "      <th>opinions</th>\n",
       "      <th>is_TA</th>\n",
       "      <th>clean_comments</th>\n",
       "      <th>clean_op_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>dgjarh</td>\n",
       "      <td>AITA for not wanting my dads new gf to sing at...</td>\n",
       "      <td>So I (28F) am getting married next year. \\n\\nI...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>{'NTA': 82, 'YTA': 1, 'ESH': 1, 'NAH': 1, 'INF...</td>\n",
       "      <td>0</td>\n",
       "      <td>*\\n\\n\\n. It's your wedding and she's a strange...</td>\n",
       "      <td>for not wanting my dads new gf to sing at my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>dgjgap</td>\n",
       "      <td>AITA for “looking poor”?</td>\n",
       "      <td>Soo my family used to be really poor but my pa...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "      <td>YTA</td>\n",
       "      <td>{'NTA': 52, 'YTA': 142, 'ESH': 10, 'NAH': 21, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>*\\n, you offered to pay them back, and they tu...</td>\n",
       "      <td>for “looking poor”? Soo my family used to be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>dgjvfw</td>\n",
       "      <td>AITA for calling my family out on their eating...</td>\n",
       "      <td>My brother is 15, turning 16 soon.  He gets no...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>{'NTA': 66, 'YTA': 8, 'ESH': 10, 'NAH': 3, 'IN...</td>\n",
       "      <td>0</td>\n",
       "      <td>*\\n. It's nice that you're trying to help but ...</td>\n",
       "      <td>for calling my family out on their eating hab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>dgk74h</td>\n",
       "      <td>AITA for using the street parking in front of ...</td>\n",
       "      <td>I strongly dislike my neighbors and that is ma...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "      <td>NTA</td>\n",
       "      <td>{'NTA': 45, 'YTA': 5, 'ESH': 7, 'NAH': 1, 'INF...</td>\n",
       "      <td>0</td>\n",
       "      <td>*\\n. If she has a problem with a legally parke...</td>\n",
       "      <td>for using the street parking in front of my n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>dgkgu6</td>\n",
       "      <td>AITA for refusing to cook my boyfriends steak ...</td>\n",
       "      <td>I’ve been with my boyfriend for a couple years...</td>\n",
       "      <td>\\nIf you want your comment to count toward jud...</td>\n",
       "      <td>YTA</td>\n",
       "      <td>{'NTA': 53, 'YTA': 64, 'ESH': 32, 'NAH': 7, 'I...</td>\n",
       "      <td>1</td>\n",
       "      <td>*\\n.\\n\\nHe could have cooked his own before th...</td>\n",
       "      <td>for refusing to cook my boyfriends steak rare...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  dgjarh  AITA for not wanting my dads new gf to sing at...   \n",
       "1  dgjgap                           AITA for “looking poor”?   \n",
       "2  dgjvfw  AITA for calling my family out on their eating...   \n",
       "3  dgk74h  AITA for using the street parking in front of ...   \n",
       "4  dgkgu6  AITA for refusing to cook my boyfriends steak ...   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  So I (28F) am getting married next year. \\n\\nI...   \n",
       "1  Soo my family used to be really poor but my pa...   \n",
       "2  My brother is 15, turning 16 soon.  He gets no...   \n",
       "3  I strongly dislike my neighbors and that is ma...   \n",
       "4  I’ve been with my boyfriend for a couple years...   \n",
       "\n",
       "                                            comments verdict  \\\n",
       "0  \\nIf you want your comment to count toward jud...     NTA   \n",
       "1  \\nIf you want your comment to count toward jud...     YTA   \n",
       "2  \\nIf you want your comment to count toward jud...     NTA   \n",
       "3  \\nIf you want your comment to count toward jud...     NTA   \n",
       "4  \\nIf you want your comment to count toward jud...     YTA   \n",
       "\n",
       "                                            opinions  is_TA  \\\n",
       "0  {'NTA': 82, 'YTA': 1, 'ESH': 1, 'NAH': 1, 'INF...      0   \n",
       "1  {'NTA': 52, 'YTA': 142, 'ESH': 10, 'NAH': 21, ...      1   \n",
       "2  {'NTA': 66, 'YTA': 8, 'ESH': 10, 'NAH': 3, 'IN...      0   \n",
       "3  {'NTA': 45, 'YTA': 5, 'ESH': 7, 'NAH': 1, 'INF...      0   \n",
       "4  {'NTA': 53, 'YTA': 64, 'ESH': 32, 'NAH': 7, 'I...      1   \n",
       "\n",
       "                                      clean_comments  \\\n",
       "0  *\\n\\n\\n. It's your wedding and she's a strange...   \n",
       "1  *\\n, you offered to pay them back, and they tu...   \n",
       "2  *\\n. It's nice that you're trying to help but ...   \n",
       "3  *\\n. If she has a problem with a legally parke...   \n",
       "4  *\\n.\\n\\nHe could have cooked his own before th...   \n",
       "\n",
       "                                       clean_op_text  \n",
       "0   for not wanting my dads new gf to sing at my ...  \n",
       "1   for “looking poor”? Soo my family used to be ...  \n",
       "2   for calling my family out on their eating hab...  \n",
       "3   for using the street parking in front of my n...  \n",
       "4   for refusing to cook my boyfriends steak rare...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "\n",
    "### Based on `op_text`, can I predict the verdict/`is_TA`? [Jump to section results.](#Part-One-(op_text)-Best-Results)\n",
    "\n",
    "[**return to top of notebook**](#Assholery-Highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = aita[['clean_op_text', 'is_TA']].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_op_text</th>\n",
       "      <th>is_TA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>for not wanting my dads new gf to sing at my ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>for “looking poor”? Soo my family used to be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>for calling my family out on their eating hab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       clean_op_text  is_TA\n",
       "0   for not wanting my dads new gf to sing at my ...      0\n",
       "1   for “looking poor”? Soo my family used to be ...      1\n",
       "2   for calling my family out on their eating hab...      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "1997     for asking my coworker to take allergy medici...\n",
      "1998     for calling the cops on my roommate's boyfrie...\n",
      "1999     for wanting my boyfriend to move back in with...\n",
      "2000     for \"faking\" an accent so people stop thinkin...\n",
      "2001     for not helping my mother out financially whe...\n",
      "Name: clean_op_text, dtype: object\n",
      "\n",
      "\n",
      "y:\n",
      "1997    0\n",
      "1998    0\n",
      "1999    0\n",
      "2000    0\n",
      "2001    0\n",
      "Name: is_TA, dtype: int64\n",
      "\n",
      "check distribution of y, are the classes unbalanced?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.804891\n",
       "1    0.195109\n",
       "Name: is_TA, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1922,) y: (1922,)\n"
     ]
    }
   ],
   "source": [
    "X = df['clean_op_text'] # must be a vector\n",
    "print('X:')\n",
    "print(X.tail())\n",
    "print()\n",
    "print()\n",
    "\n",
    "y = df['is_TA']\n",
    "print('y:')\n",
    "print(y.tail())\n",
    "print()\n",
    "print('check distribution of y, are the classes unbalanced?')\n",
    "display(y.value_counts(normalize=True))\n",
    "\n",
    "print('X:', X.shape, 'y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.804574\n",
       "1    0.195426\n",
       "Name: is_TA, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB() + CountVectorizer() with ROC_AUC Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('estimator', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'vectorizer':[CountVectorizer()],\n",
    "    'vectorizer__max_features':[100, 500],\n",
    "    'vectorizer__stop_words':[None, stopwords.words('english')],\n",
    "    'vectorizer__ngram_range':[(1,2)],\n",
    "    'estimator':[MultinomialNB()]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                  pipe_params,\n",
    "                  cv=5,\n",
    "                  verbose=2,\n",
    "                  scoring=roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   0.9s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   0.9s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   0.9s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   1.0s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   0.9s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   1.0s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   0.9s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   0.9s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   0.9s\n",
      "[CV] estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
      "[CV]  estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   28.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 s, sys: 538 ms, total: 29 s\n",
      "Wall time: 29.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "       ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'vectorizer': [CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None,\n",
       "        stop_words=[...__ngram_range': [(1, 2)], 'estimator': [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score, needs_proba=True), verbose=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after fixing the `roc_auc` scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5974929861565542\n",
      "\n",
      "Best params:\n",
      "\testimator MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\tvectorizer CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "\tvectorizer__max_features 100\n",
      "\n",
      "\tvectorizer__ngram_range (1, 2)\n",
      "\n",
      "\tvectorizer__stop_words ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "\n",
      "Training score: 0.6976147380046631\n",
      "Test score: 0.640332068832811\n",
      "Scorer: make_scorer(roc_auc_score, needs_proba=True)\n",
      "\n",
      "Baseline:\n",
      "0    0.804574\n",
      "1    0.195426\n",
      "Name: is_TA, dtype: float64\n",
      "\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted NTA</th>\n",
       "      <th>predicted TA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual NTA</td>\n",
       "      <td>340</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual TA</td>\n",
       "      <td>67</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted NTA  predicted TA\n",
       "actual NTA            340            47\n",
       "actual TA              67            27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Metric  | Score\n",
       "|--------|--------------------\n",
       "| Accuracy | 0.763 |\n",
       "| Precision | 0.365 |\n",
       "| Sensitivity | 0.287 |\n",
       "| Specificity | 0.879 |\n",
       "| Misclassification Rate | 0.237 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.640332068832811\n"
     ]
    }
   ],
   "source": [
    "gs_pipe_performance(gs, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before fixing the `roc_auc` scorer, presumably judged on what was effectively accuracy (based off of calculated ROC at threshold = one point of 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.563573063770484\n",
      "\n",
      "Best params:\n",
      "\testimator MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\tvectorizer CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "\tvectorizer__max_features 500\n",
      "\n",
      "\tvectorizer__ngram_range (1, 2)\n",
      "\n",
      "\tvectorizer__stop_words None\n",
      "\n",
      "\n",
      "Training score: 0.65171800220886\n",
      "Test score: 0.5761174336137225\n",
      "Scorer: make_scorer(roc_auc_score)\n",
      "\n",
      "Baseline:\n",
      "0    0.804574\n",
      "1    0.195426\n",
      "Name: is_TA, dtype: float64\n",
      "\n",
      "[0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted NTA</th>\n",
       "      <th>predicted TA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual NTA</td>\n",
       "      <td>273</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual TA</td>\n",
       "      <td>52</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted NTA  predicted TA\n",
       "actual NTA            273           114\n",
       "actual TA              52            42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Metric  | Score\n",
       "|--------|--------------------\n",
       "| Accuracy | 0.655 |\n",
       "| Precision | 0.269 |\n",
       "| Sensitivity | 0.447 |\n",
       "| Specificity | 0.705 |\n",
       "| Misclassification Rate | 0.345 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6192753862224422\n"
     ]
    }
   ],
   "source": [
    "gs_pipe_performance(gs, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One (`op_text`) Best Results\n",
    "\n",
    "Before fixing the `auc_roc` scorer, the best was MultinomialNB() with CountVectorizer(max_features=500, stop_words=None, ngram_range=(1,2)).\n",
    "- AUC-ROC = 0.62\n",
    "- Accuracy = 0.655\n",
    "- Precision: 0.269 (0.074 improvement over baseline of 0.195)\n",
    "\n",
    "**After fixing the `auc_roc` scorer, the best was MultinomialNB() with CountVectorizer(max_features=100, stop_words=stopwords.words('english'), ngram_range=(1,2)).**\n",
    "- AUC-ROC = 0.64\n",
    "- Accuracy = 0.763\n",
    "- **Precision: 0.365 (0.17 improvement over baseline of 0.195)**\n",
    "\n",
    "| Metric       | Old `auc_roc` Scorer | Fixed `auc_roc` Scorer     |\n",
    "|--------------|----------------------|----------------------------|\n",
    "| num_features | 500                  | 100                        |\n",
    "| stop_words   | None                 | stopwords.words('english') |\n",
    "| ngram_range  | (1,2)                | (1,2)                      |\n",
    "| best score   | 0.564                | 0.597                      |\n",
    "| train score  | 0.652                | 0.698                      |\n",
    "| test score   | 0.576                | 0.64                       |\n",
    "| accuracy     | 0.655                | 0.763                      |\n",
    "| precision    | 0.269                | 0.365                      |\n",
    "| AUC ROC      | 0.62                 | 0.64                       |\n",
    "\n",
    "[**return to top of section**](#Part-One)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two\n",
    "### Based on `clean_comments`, can I predict the verdict/`is_TA`? [Jump to section results.](#Part-Two-(comments)-Best-Results)\n",
    "\n",
    "[**return to top of notebook**](#Assholery-Highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'selftext', 'comments', 'verdict', 'opinions', 'is_TA',\n",
       "       'clean_comments', 'clean_op_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aita[['clean_comments', 'is_TA']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comments</th>\n",
       "      <th>is_TA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>*\\n\\n\\n. It's your wedding and she's a strange...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>*\\n, you offered to pay them back, and they tu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>*\\n. It's nice that you're trying to help but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      clean_comments  is_TA\n",
       "0  *\\n\\n\\n. It's your wedding and she's a strange...      0\n",
       "1  *\\n, you offered to pay them back, and they tu...      1\n",
       "2  *\\n. It's nice that you're trying to help but ...      0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "1997    *\\n. He gets to decide what medicine he takes,...\n",
      "1998    *\\n. She signed an agreement and he has no rig...\n",
      "1999    *\\n a thousand percent!! Wth is this sugar bab...\n",
      "2000    *\\n as far as I can see. You’ve found a way th...\n",
      "2001    *\\nThey disowned you. They don't get to play t...\n",
      "Name: clean_comments, dtype: object\n",
      "\n",
      "\n",
      "y:\n",
      "1997    0\n",
      "1998    0\n",
      "1999    0\n",
      "2000    0\n",
      "2001    0\n",
      "Name: is_TA, dtype: int64\n",
      "\n",
      "check distribution of y, are the classes unbalanced?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.804891\n",
       "1    0.195109\n",
       "Name: is_TA, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1922,) y: (1922,)\n"
     ]
    }
   ],
   "source": [
    "X = df['clean_comments'] # must be a vector\n",
    "print('X:')\n",
    "print(X.tail())\n",
    "print()\n",
    "print()\n",
    "\n",
    "y = df['is_TA']\n",
    "print('y:')\n",
    "print(y.tail())\n",
    "print()\n",
    "print('check distribution of y, are the classes unbalanced?')\n",
    "display(y.value_counts(normalize=True))\n",
    "\n",
    "print('X:', X.shape, 'y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.804574\n",
       "1    0.195426\n",
       "Name: is_TA, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression + CountVectorizer() **with ROC_AUC scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('estimator', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "        'vectorizer':[CountVectorizer()],\n",
    "        'vectorizer__max_features':[100, 500],\n",
    "        'vectorizer__stop_words':[None, stopwords.words('english')],\n",
    "        'vectorizer__ngram_range':[(1,2)],\n",
    "        'estimator':[LogisticRegression()]\n",
    "    }\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                 pipe_params,\n",
    "                 cv=5,\n",
    "                 verbose=2,\n",
    "                 scoring=roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  39.8s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   58.4s remaining:    0.0s\n",
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  39.4s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  39.7s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  43.7s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  45.0s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  38.8s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  37.8s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  37.5s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  37.8s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  36.7s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  44.7s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  43.7s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  45.9s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  44.8s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  47.0s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  38.1s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  39.2s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  37.7s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  38.4s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  37.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 19.3min finished\n",
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 37s, sys: 21.9 s, total: 19min 59s\n",
      "Wall time: 20min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "       ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'vectorizer': [CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "   ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score, needs_proba=True), verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after fixing the `roc_auc` scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7119710364954958\n",
      "\n",
      "Best params:\n",
      "\testimator LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "\tvectorizer CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "\tvectorizer__max_features 500\n",
      "\n",
      "\tvectorizer__ngram_range (1, 2)\n",
      "\n",
      "\tvectorizer__stop_words None\n",
      "\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.7458903733025456\n",
      "Scorer: make_scorer(roc_auc_score, needs_proba=True)\n",
      "\n",
      "Baseline:\n",
      "0    0.804574\n",
      "1    0.195426\n",
      "Name: is_TA, dtype: float64\n",
      "\n",
      "[1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted NTA</th>\n",
       "      <th>predicted TA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual NTA</td>\n",
       "      <td>344</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual TA</td>\n",
       "      <td>42</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted NTA  predicted TA\n",
       "actual NTA            344            43\n",
       "actual TA              42            52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Metric  | Score\n",
       "|--------|--------------------\n",
       "| Accuracy | 0.823 |\n",
       "| Precision | 0.547 |\n",
       "| Sensitivity | 0.553 |\n",
       "| Specificity | 0.889 |\n",
       "| Misclassification Rate | 0.177 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7458903733025456\n"
     ]
    }
   ],
   "source": [
    "gs_pipe_performance(gs, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before fixing the `roc_auc` scorer, presumably judged on what was effectively accuracy (based off of calculated ROC at threshold = one point of 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6664847908795416\n",
      "\n",
      "Best params:\n",
      "\testimator LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "\tvectorizer CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "\tvectorizer__max_features 500\n",
      "\n",
      "\tvectorizer__ngram_range (1, 2)\n",
      "\n",
      "\tvectorizer__stop_words None\n",
      "\n",
      "\n",
      "Training score: 0.998220640569395\n",
      "Test score: 0.7276513277255484\n",
      "Scorer: make_scorer(roc_auc_score)\n",
      "\n",
      "Baseline:\n",
      "0    0.804574\n",
      "1    0.195426\n",
      "Name: is_TA, dtype: float64\n",
      "\n",
      "[1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted NTA</th>\n",
       "      <th>predicted TA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual NTA</td>\n",
       "      <td>345</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual TA</td>\n",
       "      <td>41</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted NTA  predicted TA\n",
       "actual NTA            345            42\n",
       "actual TA              41            53"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Metric  | Score\n",
       "|--------|--------------------\n",
       "| Accuracy | 0.827 |\n",
       "| Precision | 0.558 |\n",
       "| Sensitivity | 0.564 |\n",
       "| Specificity | 0.891 |\n",
       "| Misclassification Rate | 0.173 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7531200175930508\n"
     ]
    }
   ],
   "source": [
    "gs_pipe_performance(gs, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two (`comments`) Best Results\n",
    "\n",
    "Before fixing the `auc_roc` scorer, the best was LogisticRegression() with CountVectorizer(max_features=500, stop_words=None, ngram_range=(1,2)). Found with `scoring=auc_roc`.\n",
    "- AUC-ROC = 0.75\n",
    "- Accuracy = 0.827\n",
    "- Precision: 0.558 (0.363 improvement over baseline of 0.195)\n",
    "\n",
    "**After fixing the `auc_roc` scorer, the best was LogisticRegression() with CountVectorizer(max_features=500, stop_words=None, ngram_range=(1,2)).**\n",
    "- AUC-ROC = 0.712\n",
    "- Accuracy = 0.823\n",
    "**- Precision: 0.547 (0.352 improvement over baseline of 0.195)**\n",
    "\n",
    "| Metric       | Old `auc_roc` Scorer | Fixed `auc_roc` Scorer |\n",
    "|--------------|----------------------|------------------------|\n",
    "| num_features | 500                  | 500                    |\n",
    "| stop_words   | None                 | None                   |\n",
    "| ngram_range  | (1,2)                | (1,2)                  |\n",
    "| best score   | 0.666                | 0.712                  |\n",
    "| train score  | 0.998                | 1.0                    |\n",
    "| test score   | 0.728                | 0.746                  |\n",
    "| accuracy     | 0.827                | 0.823                  |\n",
    "| precision    | 0.558                | 0.547                  |\n",
    "| AUC ROC      | 0.75                 | 0.75                   |\n",
    "\n",
    "[**return to top of section**](#Part-Two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three\n",
    "### Based on `op_text` *and* `clean_comments`, can I predict the verdict/`is_TA`? [Jump to section results.](#Part-Three-(all_text)-Best-Results)\n",
    "\n",
    "[**return to top of notebook**](#Assholery-Highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'selftext', 'comments', 'verdict', 'opinions', 'is_TA',\n",
       "       'clean_comments', 'clean_op_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = aita[['clean_op_text', 'clean_comments', 'is_TA']].copy()\n",
    "df['clean_comments'] = df['clean_comments'].apply(clean_post)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "      <th>is_TA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>for not wanting my dads new gf to sing at my ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>for “looking poor”? Soo my family used to be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>for calling my family out on their eating hab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            all_text  is_TA\n",
       "0   for not wanting my dads new gf to sing at my ...      0\n",
       "1   for “looking poor”? Soo my family used to be ...      1\n",
       "2   for calling my family out on their eating hab...      0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['all_text'] = df['clean_op_text'] + \" \" + df['clean_comments']\n",
    "df = df.drop(columns=['clean_op_text', 'clean_comments'])[['all_text', 'is_TA']]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "1997     for asking my coworker to take allergy medici...\n",
      "1998     for calling the cops on my roommate's boyfrie...\n",
      "1999     for wanting my boyfriend to move back in with...\n",
      "2000     for \"faking\" an accent so people stop thinkin...\n",
      "2001     for not helping my mother out financially whe...\n",
      "Name: all_text, dtype: object\n",
      "\n",
      "\n",
      "y:\n",
      "1997    0\n",
      "1998    0\n",
      "1999    0\n",
      "2000    0\n",
      "2001    0\n",
      "Name: is_TA, dtype: int64\n",
      "\n",
      "check distribution of y, are the classes unbalanced?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.804891\n",
       "1    0.195109\n",
       "Name: is_TA, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1922,) y: (1922,)\n"
     ]
    }
   ],
   "source": [
    "X = df['all_text'] # must be a vector\n",
    "print('X:')\n",
    "print(X.tail())\n",
    "print()\n",
    "print()\n",
    "\n",
    "y = df['is_TA']\n",
    "print('y:')\n",
    "print(y.tail())\n",
    "print()\n",
    "print('check distribution of y, are the classes unbalanced?')\n",
    "display(y.value_counts(normalize=True))\n",
    "\n",
    "print('X:', X.shape, 'y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.804574\n",
       "1    0.195426\n",
       "Name: is_TA, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Previous Model - Logistic Regression + CountVectorizer() **with ROC_AUC scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('estimator', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "        'vectorizer':[CountVectorizer()],\n",
    "        'vectorizer__max_features':[100, 500],\n",
    "        'vectorizer__stop_words':[None, stopwords.words('english')],\n",
    "        'vectorizer__ngram_range':[(1,2)],\n",
    "        'estimator':[LogisticRegression()]\n",
    "    }\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                 pipe_params,\n",
    "                 cv=5,\n",
    "                 verbose=2,\n",
    "                 scoring=roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  44.1s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n",
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  39.2s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  36.9s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  38.0s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  38.9s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  37.5s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  39.4s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  40.2s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  39.0s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=100, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  39.3s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  50.1s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  46.7s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  50.5s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  49.1s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=  48.7s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  42.6s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  42.0s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  42.7s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  42.9s\n",
      "[CV] estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), vectorizer=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), vectorizer__max_features=500, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], total=  39.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 20.6min finished\n",
      "/Users/shreya/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 49s, sys: 22.9 s, total: 21min 12s\n",
      "Wall time: 21min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "       ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'vectorizer': [CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "   ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score, needs_proba=True), verbose=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after fixing the `roc_auc` scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6951290484820776\n",
      "\n",
      "Best params:\n",
      "\testimator LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "\tvectorizer CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "\tvectorizer__max_features 100\n",
      "\n",
      "\tvectorizer__ngram_range (1, 2)\n",
      "\n",
      "\tvectorizer__stop_words None\n",
      "\n",
      "\n",
      "Training score: 0.826178058657504\n",
      "Test score: 0.7891033041948431\n",
      "Scorer: make_scorer(roc_auc_score, needs_proba=True)\n",
      "\n",
      "Baseline:\n",
      "0    0.804574\n",
      "1    0.195426\n",
      "Name: is_TA, dtype: float64\n",
      "\n",
      "[0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted NTA</th>\n",
       "      <th>predicted TA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual NTA</td>\n",
       "      <td>362</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual TA</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted NTA  predicted TA\n",
       "actual NTA            362            25\n",
       "actual TA              64            30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Metric  | Score\n",
       "|--------|--------------------\n",
       "| Accuracy | 0.815 |\n",
       "| Precision | 0.545 |\n",
       "| Sensitivity | 0.319 |\n",
       "| Specificity | 0.935 |\n",
       "| Misclassification Rate | 0.185 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7891033041948431\n"
     ]
    }
   ],
   "source": [
    "gs_pipe_performance(gs, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before fixing the `roc_auc` scorer, presumably judged on what was effectively accuracy (based off of calculated ROC at threshold = one point of 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6762145813575552\n",
      "\n",
      "Best params:\n",
      "\testimator LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "\n",
      "\tvectorizer CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "\tvectorizer__max_features 500\n",
      "\n",
      "\tvectorizer__ngram_range (1, 2)\n",
      "\n",
      "\tvectorizer__stop_words None\n",
      "\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.7210401891252955\n",
      "Scorer: make_scorer(roc_auc_score)\n",
      "\n",
      "Baseline:\n",
      "0    0.804574\n",
      "1    0.195426\n",
      "Name: is_TA, dtype: float64\n",
      "\n",
      "[1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Confusion Matrix"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted NTA</th>\n",
       "      <th>predicted TA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual NTA</td>\n",
       "      <td>344</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual TA</td>\n",
       "      <td>42</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted NTA  predicted TA\n",
       "actual NTA            344            43\n",
       "actual TA              42            52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Metric  | Score\n",
       "|--------|--------------------\n",
       "| Accuracy | 0.823 |\n",
       "| Precision | 0.547 |\n",
       "| Sensitivity | 0.553 |\n",
       "| Specificity | 0.889 |\n",
       "| Misclassification Rate | 0.177 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7580680631150696\n"
     ]
    }
   ],
   "source": [
    "gs_pipe_performance(gs, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Three (`all_text`) Best Results\n",
    "\n",
    "Before fixing the `auc_roc` scorer, the best was LogisticRegression() with CountVectorizer(max_features=500, stop_words=None, ngram_range=(1,2)). Found with `scoring=auc_roc`.\n",
    "- AUC-ROC = 0.76\n",
    "- Accuracy = 0.823\n",
    "- Precision: 0.547 (0.352 improvement over baseline of 0.195)\n",
    "\n",
    "**After fixing the `auc_roc` scorer, the best was LogisticRegression() with CountVectorizer(max_features=100, stop_words=None, ngram_range=(1,2)).**\n",
    "- AUC-ROC = 0.79\n",
    "- Accuracy = 0.815\n",
    "- **Precision: 0.545 (0.35 improvement over baseline of 0.195)**\n",
    "\n",
    "| Metric       \t| Old `auc_roc` Scorer \t| Fixed `auc_roc` Scorer \t|\n",
    "|--------------\t|----------------------\t|------------------------\t|\n",
    "| num_features \t| 500                  \t| 100                    \t|\n",
    "| stop_words   \t| None                 \t| None                   \t|\n",
    "| ngram_range  \t| (1,2)                \t| (1,2)                  \t|\n",
    "| best score   \t| 0.676                \t| 0.695                  \t|\n",
    "| train score  \t| 1.0                  \t| 0.826                  \t|\n",
    "| test score   \t| 0.721                \t| 0.789                  \t|\n",
    "| accuracy     \t| 0.823                \t| 0.815                  \t|\n",
    "| precision    \t| 0.547                \t| 0.545                  \t|\n",
    "| AUC ROC      \t| 0.76                 \t| 0.79                   \t|\n",
    "\n",
    "[**return to top of section**](#Part-Three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I've added tables comparing my results from before fixing the `auc_roc` scorer (so back when it was presumably just calculating based off of predictions of 0 or 1 at a single threshold of 0.5, effectively an accuracy score) and after fixing the `auc_roc` scorer (when it's actually calculating based off of the predicted probabilities). This is mainly just to satisfy my own curiosity about whether it would change the results or not -- and it does, in some cases (mainly the vectorizer hyperparameters).\n",
    "\n",
    "This does raise some more questions about which model to select as the best -- e.g., for Part Two, the returned best model after grid-searching with the `auc_roc` scorer performs marginally worse on both accuracy and precision, and has the same AUC ROC score. But I'm not going to dive too deeply into this further at this moment in time.\n",
    "\n",
    "### Part One:\n",
    "**After fixing `auc_roc` scorer, the best model is MultinomialNB() with CountVectorizer(max_features=100, stop_words=stopwords.words('english'), ngram_range=(1,2)).**\n",
    "- AUC-ROC = 0.64\n",
    "- Accuracy = 0.763\n",
    "- **Precision: 0.365 (0.17 improvement over baseline of 0.195)**\n",
    "\n",
    "| Metric       | Old `auc_roc` Scorer | Fixed `auc_roc` Scorer     |\n",
    "|--------------|----------------------|----------------------------|\n",
    "| num_features | 500                  | 100                        |\n",
    "| stop_words   | None                 | stopwords.words('english') |\n",
    "| ngram_range  | (1,2)                | (1,2)                      |\n",
    "| best score   | 0.564                | 0.597                      |\n",
    "| train score  | 0.652                | 0.698                      |\n",
    "| test score   | 0.576                | 0.64                       |\n",
    "| accuracy     | 0.655                | 0.763                      |\n",
    "| precision    | 0.269                | 0.365                      |\n",
    "| AUC ROC      | 0.62                 | 0.64                       |\n",
    "\n",
    "### Part Two:\n",
    "**After fixing `auc_roc` scorer, the best model is LogisticRegression() with CountVectorizer(max_features=500, stop_words=None, ngram_range=(1,2)).**\n",
    "- AUC-ROC = 0.75\n",
    "- Accuracy = 0.823\n",
    "- **Precision: 0.547 (0.352 improvement over baseline of 0.195)**\n",
    "\n",
    "| Metric       | Old `auc_roc` Scorer | Fixed `auc_roc` Scorer |\n",
    "|--------------|----------------------|------------------------|\n",
    "| num_features | 500                  | 500                    |\n",
    "| stop_words   | None                 | None                   |\n",
    "| ngram_range  | (1,2)                | (1,2)                  |\n",
    "| best score   | 0.666                | 0.712                  |\n",
    "| train score  | 0.998                | 1.0                    |\n",
    "| test score   | 0.728                | 0.746                  |\n",
    "| accuracy     | 0.827                | 0.823                  |\n",
    "| precision    | 0.558                | 0.547                  |\n",
    "| AUC ROC      | 0.75                 | 0.75                   |\n",
    "\n",
    "### Part Three:\n",
    "**After fixing `auc_roc` scorer, the best model is LogisticRegression() with CountVectorizer(max_features=100, stop_words=None, ngram_range=(1,2)).**\n",
    "- AUC-ROC = 0.79\n",
    "- Accuracy = 0.815\n",
    "- **Precision: 0.545 (0.35 improvement over baseline of 0.195)**\n",
    "\n",
    "| Metric       \t| Old `auc_roc` Scorer \t| Fixed `auc_roc` Scorer \t|\n",
    "|--------------\t|----------------------\t|------------------------\t|\n",
    "| num_features \t| 500                  \t| 100                    \t|\n",
    "| stop_words   \t| None                 \t| None                   \t|\n",
    "| ngram_range  \t| (1,2)                \t| (1,2)                  \t|\n",
    "| best score   \t| 0.676                \t| 0.695                  \t|\n",
    "| train score  \t| 1.0                  \t| 0.826                  \t|\n",
    "| test score   \t| 0.721                \t| 0.789                  \t|\n",
    "| accuracy     \t| 0.823                \t| 0.815                  \t|\n",
    "| precision    \t| 0.547                \t| 0.545                  \t|\n",
    "| AUC ROC      \t| 0.76                 \t| 0.79                   \t|\n",
    "\n",
    "[**return to top of notebook**](#Assholery-Highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
